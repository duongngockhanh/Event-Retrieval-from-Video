{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tyk_2IosWEwUFf_dwW70EcUdyvpJt2PB","timestamp":1694225970783},{"file_id":"1lFI2U9w_8wcAWlyOQISNvUkgAegf59Xf","timestamp":1694224935206}],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#1. Kết nối Drive và set up các thư viện"],"metadata":{"id":"ECjqPOgCHB1u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmnWtRAL_fKL"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install ftfy regex tqdm -q\n","!pip install git+https://github.com/openai/CLIP.git -q"],"metadata":{"id":"JGnbYK0_FlEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","import time\n","import torch\n","import clip\n","import os, glob\n","from PIL import Image\n","from tqdm import tqdm\n","from numpy.linalg import svd\n","from PIL import Image\n","# import matplotlib.pyplot as plt\n","# import torch.nn.functional as F"],"metadata":{"id":"fqdsY_olFHN_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2. Clone TransNet và giải nén List Videos"],"metadata":{"id":"cxn2PTS3HK00"}},{"cell_type":"markdown","source":["##2.1. Làm việc trong Drive"],"metadata":{"id":"SvBm6e0ipyx2"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Full_Folder\n","# Đảm bảo rằng đã có Videos_L10.zip trong Full_Folder"],"metadata":{"id":"ahFDNTK3BXS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/soCzech/TransNetV2.git\n","%cd TransNetV2\n","!python setup.py install\n","!pip install ffmpeg-python"],"metadata":{"id":"rzznSI4tqkbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Need to change\n","!unzip -q /content/drive/MyDrive/Full_Folder/Videos_L10.zip -d ./"],"metadata":{"id":"-pzKsVRWA80p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.2. Làm việc ngoài Drive"],"metadata":{"id":"H61LcFuDp4mt"}},{"cell_type":"code","source":["%cd /content/\n","!mkdir Full_Folder\n","%cd Full_Folder\n","\n","# 2. Need to change\n","# https://drive.google.com/file/d/1Vq5FX_w90A5PSfJRDSiZh4bbeVFHk0Xh/view?usp=sharing\n","!gdown 1Vq5FX_w90A5PSfJRDSiZh4bbeVFHk0Xh"],"metadata":{"id":"QxZbcHTKp8Gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/soCzech/TransNetV2.git\n","%cd TransNetV2\n","!python setup.py install\n","!pip install ffmpeg-python"],"metadata":{"id":"BElrMGZFqRrX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. Need to change\n","!unzip -q /content/Full_Folder/Videos_L10.zip -d ./"],"metadata":{"id":"gdxmfRzEp-sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["science = \"L10\" # 4. Need to change"],"metadata":{"id":"ZaQNwJezt0SD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. TransNet inference"],"metadata":{"id":"HQ-Hf2BHHa6j"}},{"cell_type":"code","source":["from inference.transnetv2 import TransNetV2\n","model = TransNetV2(model_dir=\"inference/transnetv2-weights/\")"],"metadata":{"id":"8vWN0ATNAj_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["video_dir = \"video\"\n","general_save_dir = f\"{science}_keyframes\"\n","\n","video_list = sorted(os.listdir(video_dir))\n","\n","\n","for single_video in video_list:\n","    start_time = time.time()\n","    single_video_path = os.path.join(video_dir, single_video)\n","    video_frames, single_frame_predictions, all_frame_predictions = model.predict_video(single_video_path)\n","    res_arr = model.predictions_to_scenes(single_frame_predictions)\n","\n","    full_list = [np.insert(i, 1, int(i.mean())) for i in res_arr]\n","    full_arr = np.array(full_list)\n","\n","    saved_dir = f\"{general_save_dir}/{single_video[:-4]}\"\n","    os.makedirs(saved_dir, exist_ok=True)\n","\n","    cap = cv2.VideoCapture(single_video_path)\n","\n","    if not cap.isOpened():\n","        print(\"Can't open the video.\")\n","        exit()\n","\n","    for idx, segment in enumerate(full_arr):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, segment[0])\n","        _, frame = cap.read()\n","        saved_path = f\"{saved_dir}/{idx:03d}a_{segment[0]:05d}.jpg\"\n","        cv2.imwrite(saved_path, frame)\n","\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, segment[1])\n","        _, frame = cap.read()\n","        saved_path = f\"{saved_dir}/{idx:03d}b_{segment[1]:05d}.jpg\"\n","        cv2.imwrite(saved_path, frame)\n","\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, segment[2])\n","        _, frame = cap.read()\n","        saved_path = f\"{saved_dir}/{idx:03d}c_{segment[2]:05d}.jpg\"\n","        cv2.imwrite(saved_path, frame)\n","\n","    cap.release()\n","    iteration_time = round(time.time() - start_time, 2)\n","    print(f\"Save successfully {saved_dir} in {iteration_time}s\")\n","    print()"],"metadata":{"id":"Iv4KtUmuB-9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5. Need to change\n","!zip -r L10_keyframes.zip L10_keyframes"],"metadata":{"id":"ckIwsiWgCBmQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#4. CLIP inference"],"metadata":{"id":"87kMn4taHwS6"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)"],"metadata":{"id":"F0blBHT0EuJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","\n","src_dir = f\"{science}_keyframes\"\n","\n","save_clip_dir = f\"clip_features_{science}\"\n","os.makedirs(save_clip_dir, exist_ok=True)\n","\n","videos = sorted(os.listdir(src_dir))\n","\n","for video in videos:\n","    print(video)\n","    clip_vector_list = []\n","    video_path = os.path.join(src_dir, video)\n","    images = sorted(os.listdir(video_path))\n","    for image in tqdm(images):\n","        image_path = os.path.join(video_path, image)\n","        image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            image_features = model.encode_image(image_input).cpu().detach().numpy()\n","            clip_vector_list.append(image_features)\n","\n","    clip_vector_arr = np.array(clip_vector_list)\n","    save_path = f\"{save_clip_dir}/{video}.npy\"\n","    np.save(save_path, clip_vector_arr)\n","    print(f\"Save successfully {save_path}\")\n","    print()\n","\n","\n","print(time.time() - start_time)"],"metadata":{"id":"r_DMbHLAEy7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6. Need to change\n","!zip -r compressed_L10.zip compressed_L10"],"metadata":{"id":"WKif8IwuFEc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#5. TransNet post-processing"],"metadata":{"id":"3tCrYr8UILM9"}},{"cell_type":"code","source":["def compress_image(img_name, k):\n","    # Load the image and move it to GPU\n","    img = np.asarray(Image.open(img_name))\n","    img_tensor = torch.tensor(img, dtype=torch.float32).cuda()\n","\n","    # Split the image into RGB channels and move to GPU\n","    r = img_tensor[:, :, 0]\n","    g = img_tensor[:, :, 1]\n","    b = img_tensor[:, :, 2]\n","\n","    # Perform SVD on each channel using GPU-accelerated torch.svd\n","    ur, sr, vr = torch.svd(r, some=True)\n","    ug, sg, vg = torch.svd(g, some=True)\n","    ub, sb, vb = torch.svd(b, some=True)\n","\n","    # Compress the channels on GPU\n","    rr = torch.matmul(ur[:, :k], torch.matmul(torch.diag(sr[:k]), vr[:, :k].t()))\n","    rg = torch.matmul(ug[:, :k], torch.matmul(torch.diag(sg[:k]), vg[:, :k].t()))\n","    rb = torch.matmul(ub[:, :k], torch.matmul(torch.diag(sb[:k]), vb[:, :k].t()))\n","\n","    # Create the compressed image tensor\n","    rimg = torch.zeros_like(img_tensor)\n","    rimg[:, :, 0] = rr\n","    rimg[:, :, 1] = rg\n","    rimg[:, :, 2] = rb\n","\n","    # Ensure pixel values are within the valid range\n","    rimg = torch.clamp(rimg, 0, 255)\n","\n","    # Convert the compressed image back to a NumPy array\n","    compressed_image = rimg.cpu().numpy().astype(np.uint8)\n","    return compressed_image\n","\n","def resize_img(img, size=(255,144)):\n","    img = Image.fromarray(img)\n","    img = img.resize(size)\n","    return img"],"metadata":{"id":"W5Jy7ZsSJxY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main\n","\n","# src_dir = \"L10_keyframes\"\n","# videos = sorted(os.listdir(src_dir))\n","\n","compressed_dir = f'compressed_{science}' # 9. Need to change\n","\n","for video in videos:\n","    video_path = os.path.join(src_dir, video)\n","    for single_image in tqdm(sorted(os.listdir(video_path))):\n","        compressed_img = compress_image(os.path.join(video_path, single_image), 100)\n","        img = resize_img(compressed_img)\n","\n","        saved_video_path = os.path.join(compressed_dir, video)\n","        os.makedirs(saved_video_path, exist_ok=True)\n","\n","        img.save(os.path.join(saved_video_path, single_image)[:-4] + '.webp', 'webp')"],"metadata":{"id":"8iGmZes-i4rw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7. Need to change\n","!zip -r compressed_L10.zip compressed_L10"],"metadata":{"id":"v5W5SJD1lXoh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#6. YOLOv8 inference"],"metadata":{"id":"tuZIPsIIITtY"}},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"id":"ZGscixdDlg9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ultralytics import YOLO\n","model = YOLO(\"yolov8n.pt\")"],"metadata":{"id":"Ss3J2HsOmk2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# src_dir = \"L10_keyframes\"\n","# videos = sorted(os.listdir(src_dir))\n","\n","saved_detection_dir = f\"detection_{science}\"\n","\n","for video in videos:\n","    second_path = os.path.join(src_dir, video)\n","    img_list = sorted(os.listdir(second_path))\n","\n","    for img_name in img_list:\n","        img_path = os.path.join(second_path, img_name)\n","        img = cv2.imread(img_path)\n","\n","        results = model.predict(img)\n","\n","        saved_folder = os.path.join(saved_detection_dir, video)\n","        os.makedirs(saved_folder)\n","\n","        label_path = os.path.join(saved_folder, img_name)[:-4] + \".txt\"\n","        with open(label_path, \"w\") as f:\n","            for r in results:\n","                boxes = r.boxes\n","                for box in boxes:\n","                    list_box = (box.xywhn[0]).tolist()\n","                    name = int((box.cls).item())\n","                    label = str(name) + \" \" + str(list_box[0]) + \" \" + str(list_box[1]) + \" \" + str(list_box[2]) + \" \" + str(list_box[3])\n","                    f.write(label)\n","                    f.write(\"\\n\")"],"metadata":{"id":"ARhUEHsQmoW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r detection_L10.zip detection_L10 #8. Need to change"],"metadata":{"id":"psiqAJTauKeI"},"execution_count":null,"outputs":[]}]}