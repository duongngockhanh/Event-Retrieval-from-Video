{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNUPXSnPY4KOKsUGjKhU1nM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNO2ajypiHLJ","executionInfo":{"status":"ok","timestamp":1693922216160,"user_tz":-420,"elapsed":14701,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"}},"outputId":"068dd577-0575-4b0a-e0a5-5a885ccf9980"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install ftfy regex tqdm -q\n","!pip install git+https://github.com/openai/CLIP.git -q"]},{"cell_type":"code","source":["import torch\n","import clip\n","from PIL import Image\n","import os, glob\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0co7OIyiarf","executionInfo":{"status":"ok","timestamp":1693922236944,"user_tz":-420,"elapsed":20786,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"}},"outputId":"82241e4b-efed-4158-a1d9-ff0012bc589d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 62.5MiB/s]\n"]}]},{"cell_type":"code","source":["!gdown 1f5r2QEntqOkF9snjYtDI0Kdi3PtKpZMM\n","!unzip custom_keyframes.zip\n","!rm custom_keyframes.zip"],"metadata":{"id":"F6nrGid7h7Tq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import clip\n","from PIL import Image\n","import os, glob\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","\n","\n","start_time = time.time()\n","\n","src_dir = \"custom_keyframes_L01\"\n","\n","save_clip_dir = \"clip_features_L01\"\n","os.makedirs(save_clip_dir, exist_ok=True)\n","\n","\n","videos = sorted(os.listdir(src_dir))\n","\n","for video in videos:\n","    print(video)\n","    clip_vector_list = []\n","    video_path = os.path.join(src_dir, video)\n","    images = sorted(os.listdir(video_path))\n","    for image in tqdm(images):\n","        image_path = os.path.join(video_path, image)\n","        image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            image_features = model.encode_image(image_input)\n","            image_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n","            clip_vector_list.append(image_norm.cpu().data.numpy())\n","\n","    clip_vector_arr = np.array(clip_vector_list)\n","    save_path = f\"{save_clip_dir}/{video}.npy\"\n","    np.save(save_path, clip_vector_arr)\n","    print(f\"Save successfully {save_path}\")\n","\n","print()\n","print(time.time() - start_time)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukLqW2ygix4Z","outputId":"76b04a60-000a-48bf-9094-6a0dd20bf95c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["L01_V001\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 807/807 [00:21<00:00, 37.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Save successfully clip_features_L01/L01_V001.npy\n","L01_V002\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 490/882 [00:12<00:09, 42.20it/s]"]}]}]}