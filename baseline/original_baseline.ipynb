{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fcc581b",
   "metadata": {
    "id": "5fcc581b"
   },
   "source": [
    "# Hướng dẫn truy vấn dữ liệu thị giác dùng fiftyone\n",
    "\n",
    "Đây là hướng dẫn dùng cho các đội tham dự AI Challenge 2023. Hướng dẫn này nhằm mục đích giới thiệu cho các đội một phương pháp cơ bản để truy vấn dữ liệu dựa trên thông tin BTC cung cấp và giới thiệu công cụ fiftyone để hỗ trợ đội thi đánh giá kết quả.\n",
    "\n",
    "## Cài đặt ban đầu\n",
    "\n",
    "Bạn cần cài đặt môi trường để chạy được notebook này trên máy tính cá nhân của bạn. Hướng dẫn này không bao gồm phần cài đặt môi trường. Khuyến nghị: các bạn có thể cài đặt [Anaconda](https://docs.anaconda.com/free/anaconda/install/windows/).\n",
    "\n",
    "## Cài đặt các thư viện FiftyOne và PyTorch\n",
    "Hướng dẫn này dùng fiftyone là công cụ để trực quan dữ liệu và pytorch là backend chính cho các thuật toán máy học.\n",
    "\n",
    "### Lưu ý: Đối với các bạn dùng Windows nên dùng bản fiftyone **v0.21.4**, không nên dùng bản mới nhất!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9afb40",
   "metadata": {
    "id": "fb9afb40"
   },
   "source": [
    "Load dữ liệu keyframe từ thư mục chứa keyframe. Mỗi ảnh và thông tin đi kèm sau này sẽ được lưu trữ trong một Sample. Tất cả các Sample được lưu trong Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d929e-6678-4c02-90d0-df4a8bff1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fiftyone==0.21.4\n",
    "! pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "357d4489",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 63974,
     "status": "error",
     "timestamp": 1692878838677,
     "user": {
      "displayName": "Khanh Duong",
      "userId": "02272471262918357102"
     },
     "user_tz": -420
    },
    "id": "357d4489",
    "outputId": "c4d5d764-3f20-4043-fa5c-893c511abbd5"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7ba43",
   "metadata": {
    "id": "36b7ba43"
   },
   "source": [
    "Load dữ liệu keyframe từ thư mục chứa keyframe. Trong hướng dẫn này tất cả các file Keyframes_L*.zip được giải nén vào thư mục `D:\\AIC\\Keyframes`. Mỗi ảnh và thông tin đi kèm sau này sẽ được lưu trữ trong một `Sample`. Tất cả các `Sample` được lưu trong `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e072688",
   "metadata": {
    "id": "9e072688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 7658/7658 [1.7s elapsed, 0s remaining, 4.6K samples/s]         \n"
     ]
    }
   ],
   "source": [
    "dataset = fo.Dataset.from_images_dir('original_data/keyframes', name=None, tags=None, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89e71c",
   "metadata": {
    "id": "9b89e71c"
   },
   "source": [
    "Sau khi dữ liệu đã load lên xong. Bạn có thể truy cập vào đường vào ứng dụng web của fiftyone từ [http://localhost:5151](http://localhost:5151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58be11b8",
   "metadata": {
    "id": "58be11b8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n",
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, auto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89226552-c85a-49e5-a01c-3c6900457741",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ce744",
   "metadata": {
    "id": "078ce744"
   },
   "source": [
    "Hoặc bạn có thể chạy cell bên dưới để mở tab mới cho ứng dụng web fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bce0d9ce",
   "metadata": {
    "id": "bce0d9ce"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "window.open('http://localhost:5151/');",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session.open_tab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28298924",
   "metadata": {
    "id": "28298924"
   },
   "source": [
    "### Trích xuất thêm thông tin tên của video và frameid\n",
    "Thông tin `video` và `frameid` sẽ được lấy từ tên của tập tin keyframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ee02ad",
   "metadata": {
    "id": "a8ee02ad"
   },
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    _, sample['video'], sample['frameid'] = sample['filepath'][:-4].rsplit('/', 2)\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d66008",
   "metadata": {
    "id": "11d66008"
   },
   "source": [
    "Bạn có thể xem `Sample` đầu tiên của `Dataset` bằng lệnh sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6997ef1c",
   "metadata": {
    "id": "6997ef1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sample: {\n",
      "    'id': '64ed6801c353dcf468117efa',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/Users/khanhmac/Desktop/hcmai-2023/baseline/data/keyframes/L01_V001/0001.jpg',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309a34f",
   "metadata": {
    "id": "4309a34f"
   },
   "source": [
    "### Thêm thông tin kết quả của object detection.\n",
    "\n",
    "Bước này có thể tốn của bạn nhiều thời gian để đọc hết tất cả các dữ liệu về object detection. Bạn có thể bỏ qua cell này và chạy cell này sau nếu muốn thử thêm các thông tin về vector CLIP embedding trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1c4395",
   "metadata": {
    "id": "ad1c4395"
   },
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    object_path = f\"original_data/objects/{sample['video']}/{sample['frameid']}.json\"\n",
    "    with open(object_path) as jsonfile:\n",
    "        det_data = json.load(jsonfile)\n",
    "    detections = []\n",
    "    for cls, box, score in zip(det_data['detection_class_entities'], det_data['detection_boxes'], det_data['detection_scores']):\n",
    "        # Convert to [top-left-x, top-left-y, width, height]\n",
    "        boxf = [float(box[1]), float(box[0]), float(box[3]) - float(box[1]), float(box[2]) - float(box[0])]\n",
    "        scoref = float(score)\n",
    "\n",
    "        # Only add objects with confidence > 0.4\n",
    "        if scoref > 0.4:\n",
    "            detections.append(\n",
    "                fo.Detection(\n",
    "                    label=cls,\n",
    "                    bounding_box= boxf,\n",
    "                    confidence=float(score)\n",
    "                )\n",
    "            )\n",
    "    sample[\"object_faster_rcnn\"] = fo.Detections(detections=detections)\n",
    "    sample.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287dbb4",
   "metadata": {
    "id": "e287dbb4"
   },
   "source": [
    "### Thêm thông tin CLIP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d5d9ca",
   "metadata": {
    "id": "58d5d9ca"
   },
   "outputs": [],
   "source": [
    "all_keyframe = glob('original_data/keyframes/*/*.jpg')\n",
    "video_keyframe_dict = {}\n",
    "all_video = glob('original_data/keyframes/*')\n",
    "all_video = [v.rsplit('/',1)[-1] for v in all_video]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53460b70",
   "metadata": {
    "id": "53460b70"
   },
   "source": [
    "Đọc thông tin clip embedding được cung cấp.\n",
    "\n",
    "Lưu ý: Các bạn cần tải đúng bản CLIP embedding từ model **CLIP ViT-B/32**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb927629",
   "metadata": {
    "id": "cb927629"
   },
   "source": [
    "Tạo dictionary `video_keyframe_dict` với `video_keyframe_dict[video]` thông tin danh sách `keyframe` của `video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f33da133",
   "metadata": {
    "id": "f33da133"
   },
   "outputs": [],
   "source": [
    "for kf in all_keyframe:\n",
    "    _, vid, kf = kf[:-4].rsplit('/',2)\n",
    "    if vid not in video_keyframe_dict.keys():\n",
    "        video_keyframe_dict[vid] = [kf]\n",
    "    else:\n",
    "        video_keyframe_dict[vid].append(kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faefe0bf",
   "metadata": {
    "id": "faefe0bf"
   },
   "source": [
    "Do thông tin vector CLIP embedding được cung cấp được lưu theo từng video nhầm mục đích tối ưu thời gian đọc dữ liệu. Cần sort lại danh sách `keyframe` của từng `video` để đảm bảo thứ tự đọc đúng với vector embedding được cung cấp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0fad7a",
   "metadata": {
    "id": "4b0fad7a"
   },
   "outputs": [],
   "source": [
    "for k,v in video_keyframe_dict.items():\n",
    "    video_keyframe_dict[k] = sorted(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99d121",
   "metadata": {
    "id": "9d99d121"
   },
   "source": [
    "Tạo dictionary `embedding_dict` với `embedding_dict[video][keyframe]` lưu thông tin vector CLIP embedding của `keyframe` trong `video` tương ứng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d94d7b",
   "metadata": {
    "id": "a5d94d7b"
   },
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "for v in all_video:\n",
    "    clip_path = f'original_data/clip-features-vit-b32/{v}.npy'\n",
    "    a = np.load(clip_path)\n",
    "    embedding_dict[v] = {}\n",
    "    for i,k in enumerate(video_keyframe_dict[v]):\n",
    "        embedding_dict[v][k] = a[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea0432e",
   "metadata": {
    "id": "5ea0432e"
   },
   "source": [
    "Tạo danh sách `clip_embedding` ứng với danh sách `sample` trong `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad16b5c",
   "metadata": {
    "id": "1ad16b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7658, 512)\n"
     ]
    }
   ],
   "source": [
    "clip_embeddings = []\n",
    "for sample in dataset:\n",
    "    clip_embedding = embedding_dict[sample['video']][sample['frameid']]\n",
    "    clip_embeddings.append(clip_embedding)\n",
    "\n",
    "\n",
    "temp_sc = np.array(clip_embeddings)\n",
    "print(temp_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655d2feb",
   "metadata": {
    "id": "655d2feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'...\n",
      " 100% |██████|    2.6Gb/2.6Gb [2.9m elapsed, 0s remaining, 21.2Mb/s]      \n",
      "Downloading CLIP tokenizer...\n",
      " 100% |█████|   10.4Mb/10.4Mb [224.3ms elapsed, 0s remaining, 46.1Mb/s]      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.internal.core.sklearn.SklearnSimilarityIndex at 0x7f93f576a250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"clip-vit-base32-torch\",      # store model's name for future use\n",
    "    embeddings=clip_embeddings,          # precomputed image embeddings\n",
    "    brain_key=\"img_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99223338",
   "metadata": {
    "id": "99223338"
   },
   "source": [
    "## Từ đây các bạn có thể thử các tính năng search, filter trên ứng dụng fiftyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b52a1291-33b4-4238-9885-63c619e6b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages (from umap-learn) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages (from umap-learn) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages (from umap-learn) (1.10.1)\n",
      "Collecting numba>=0.49 (from umap-learn)\n",
      "  Obtaining dependency information for numba>=0.49 from https://files.pythonhosted.org/packages/d2/1c/4f3e6cb644811a8e6a3e0a8dccf1457ad05ee238c507a990521fb3c92acf/numba-0.57.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading numba-0.57.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm (from umap-learn)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.49->umap-learn)\n",
      "  Obtaining dependency information for llvmlite<0.41,>=0.40.0dev0 from https://files.pythonhosted.org/packages/b8/43/50bc2dbce04b020d635438f9cb026c56bfd3a01b40a9d1d2577521a25239/llvmlite-0.40.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading llvmlite-0.40.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: importlib-metadata in /Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (6.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.16.2)\n",
      "Downloading numba-0.57.1-cp38-cp38-macosx_10_9_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.40.1-cp38-cp38-macosx_10_9_x86_64.whl (30.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: umap-learn, pynndescent\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82813 sha256=b2e2752784fb002c48b085937b97b91f5427b784156e4249ddf237eb580ea2b4\n",
      "  Stored in directory: /Users/khanhmac/Library/Caches/pip/wheels/a9/3a/67/06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55623 sha256=c057a8f63c51c51f0117574a44334f45e3889d661b6b857b96da38bef7e9e843\n",
      "  Stored in directory: /Users/khanhmac/Library/Caches/pip/wheels/f8/32/54/fa1fd0454e0441eaa32550ea2a86da3f64eb7c8e5a82436770\n",
      "Successfully built umap-learn pynndescent\n",
      "Installing collected packages: tqdm, llvmlite, numba, pynndescent, umap-learn\n",
      "Successfully installed llvmlite-0.40.1 numba-0.57.1 pynndescent-0.5.10 tqdm-4.66.1 umap-learn-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46605386",
   "metadata": {
    "id": "46605386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/khanhmac/miniconda3/envs/py38/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-08-30 06:11:18.696689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP( verbose=True)\n",
      "Wed Aug 30 06:11:41 2023 Construct fuzzy simplicial set\n",
      "Wed Aug 30 06:11:41 2023 Finding Nearest Neighbors\n",
      "Wed Aug 30 06:11:41 2023 Building RP forest with 9 trees\n",
      "Wed Aug 30 06:11:48 2023 NN descent for 13 iterations\n",
      "\t 1  /  13\n",
      "\t 2  /  13\n",
      "\t 3  /  13\n",
      "\t 4  /  13\n",
      "\t 5  /  13\n",
      "\tStopping threshold met -- exiting after 5 iterations\n",
      "Wed Aug 30 06:12:06 2023 Finished Nearest Neighbor Search\n",
      "Wed Aug 30 06:12:09 2023 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef76e20bbe44347a7a499bc422df46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 30 06:12:18 2023 Finished embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.visualization.VisualizationResults at 0x7f9403df60a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bạn cần phải cài version umap-learn hỗ trợ.\n",
    "# fob.compute_visualization(\n",
    "#     dataset,\n",
    "#     embeddings=clip_embeddings,\n",
    "#     brain_key=\"img_viz\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381972d8",
   "metadata": {
    "id": "381972d8"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
